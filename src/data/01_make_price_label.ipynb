{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nasq_ticker_name = ['AAPL', 'ADBE', 'ADI', 'ADP', 'AMAT', 'AMD', 'AMGN', 'AMZN', 'BKNG', 'COST', 'CSCO', 'CSX', 'FISV', 'GILD', 'HON', 'INTC', 'INTU', 'ISRG', 'LRCX', 'MSFT', 'MU', 'NFLX', 'ORLY', 'PEP', 'QCOM', 'REGN', 'SBUX', 'SNPS', 'TXN', 'VRTX'] # nasdaq top 30\n",
    "\n",
    "djia_ticker_name =  ['UNH', 'GS', 'MSFT', 'HD', 'MCD', 'AMGN', 'CAT', 'BA', 'HON', \n",
    "                    'CRM', 'TRV', 'AAPL', 'CVX', 'JNJ', 'AXP', 'PG', 'WMT', 'JPM', 'NKE', \n",
    "                    'IBM', 'MRK', 'MMM', 'DIS', 'KO', 'CSCO', 'VZ', 'INTC']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code usage guide\n",
    "1. First, Gather new market data and make same with stockdata/final_data\n",
    "2. Modify the block below: the market_list and file name list (nasdaq_list, djia_list)\n",
    "3. and run below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAPL', 'ADBE', 'ADI', 'ADP', 'AMAT', 'AMD', 'AMGN', 'AMZN', 'BKNG', 'COST', 'CSCO', 'CSX', 'FISV', 'GILD', 'HON', 'INTC', 'INTU', 'ISRG', 'LRCX', 'MSFT', 'MU', 'NFLX', 'ORLY', 'PEP', 'QCOM', 'REGN', 'SBUX', 'SNPS', 'TXN', 'VRTX']\n"
     ]
    }
   ],
   "source": [
    "# Set the directory\n",
    "BASE_DIR = os.path.abspath('').split('/src')[0]\n",
    "DATA_DIR = os.path.abspath('').split('/trading')[0]+'/famafrench_data'\n",
    "market_list = ['NASDAQ', 'DJIA']\n",
    "market = market_list[1]\n",
    "\n",
    "nasdaq_list = sorted([i.split('.csv')[0] for i in os.listdir(f'{DATA_DIR}/stockdata/final_data/NASDAQ') if i.endswith(\".csv\")])\n",
    "djia_list = sorted([i.split('.csv')[0] for i in os.listdir(f'{DATA_DIR}/stockdata/final_data/DJIA') if i.endswith(\".csv\")])\n",
    "\n",
    "# exclude 4 stocks (due to lack of data)\n",
    "# for i in [\"DOW\", \"V\", \"WBA\", \"CRM\"]: djia_list.remove(i)\n",
    "\n",
    "# price_filename = f\"{DATA_DIR}/stockdata/label_data/{market}/x/stock_2d_actor.csv\"\n",
    "# label_directory = f\"{DATA_DIR}/stockdata/label_data/{market_list}\"\n",
    "\n",
    "os.makedirs(f'{DATA_DIR}/stockdata/label_data/{market}/x/', exist_ok=True)\n",
    "os.makedirs(f'{DATA_DIR}/stockdata/label_data/{market}/y/', exist_ok=True)\n",
    "\n",
    "print(nasdaq_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all the close price data\n",
    "def make_2D_pretrain_data(\n",
    "                        stocks_subset:list,\n",
    "                        initial_date: str,\n",
    "                        final_date: str,\n",
    "                        save_option = True,\n",
    "                        country = 'USA',\n",
    "                        ):\n",
    "    ''' \n",
    "    Args\n",
    "    data_dir : 자산 데이터가 있는 위치\n",
    "    stocks_subset : 포트폴리오 자산명이 들어있는 리스트\n",
    "    Out\n",
    "    2d numpy size : (time, asset*features)\n",
    "    '''\n",
    "    initial_date = datetime.strptime(initial_date, '%Y-%m-%d')\n",
    "    final_date = datetime.strptime(final_date, '%Y-%m-%d')\n",
    "\n",
    "    # ticker 정보가 있는 txt파일을 열어서, 편입될 종목의 이름 수\n",
    "    # define data container\n",
    "    data_container = {}\n",
    "    y_container = {}\n",
    "\n",
    "    for ticker in stocks_subset:\n",
    "        # for key, value in pretrain_json[\"pretrain\"].items() :\n",
    "            # if ticker in value:\n",
    "        data = pd.read_csv(f'{DATA_DIR}/stockdata/final_data/{country}/{ticker}.csv',\n",
    "                            parse_dates=['date'], index_col=0)\n",
    "        data.columns = [f'close_{ticker}', f'open_{ticker}', f'high_{ticker}', f'low_{ticker}', \n",
    "                        f'volume_{ticker}', f'BTM_{ticker}',f'MarketCap_{ticker}']\n",
    "        data.sort_index(ascending=True, inplace=True)\n",
    "        # print(initial_date)\n",
    "        # print(final_date)\n",
    "        # print(len(data.loc[initial_date:final_date]))\n",
    "        data_container[ticker] = data.loc[initial_date:final_date]\n",
    "        \n",
    "\n",
    "        # making up and down labels (y_data)\n",
    "        next_p = data[f'close_{ticker}'].shift(-1).fillna(method='ffill')\n",
    "        _r = next_p/data[f'close_{ticker}']-1\n",
    "        y_container[ticker] = pd.DataFrame([1 if i>=0 else 0 for i in _r ], index=data.index) ## 정답지\n",
    "        \n",
    "        print(ticker, data[f'close_{ticker}'].index[0], data[f'close_{ticker}'].index[-1],\n",
    "        len(data[f'close_{ticker}']), len(next_p), len(_r), len(y_container[ticker]))\n",
    "        print()\n",
    "\n",
    "    \n",
    "    # Make data to the dataframe\n",
    "    final_data = pd.DataFrame()\n",
    "    y_data = pd.DataFrame()\n",
    "\n",
    "    for tic in data_container:\n",
    "        # 옆으로 붙여야 의미가 맞지 않을까?\n",
    "        final_data = pd.concat([final_data, data_container[tic]], axis=1, join='outer')\n",
    "        y_data = pd.concat([y_data, y_container[tic]], axis=1, join='outer')\n",
    "    \n",
    "    # set the labels\n",
    "    y_data.columns = stocks_subset\n",
    "    \n",
    "    time.sleep(1)\n",
    "    final_data.sort_index(ascending=True, inplace=True)\n",
    "    y_data.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "    final_data.fillna(method='ffill', inplace=True)\n",
    "    y_data.fillna(method='ffill', inplace=True)\n",
    "    #---------------------------------------------------------------------------------------------------\n",
    "\n",
    "    if save_option:\n",
    "        final_data.to_csv(f'{DATA_DIR}/stockdata/label_data/{country}/x/stock_2d_actor.csv')\n",
    "        y_data.to_csv(f'{DATA_DIR}/stockdata/label_data/{country}/y/stock_2d_y_actor.csv') # updown\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "    # return final_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "ADBE 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "ADI 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "ADP 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "AMAT 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "AMD 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "AMGN 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "AMZN 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "BKNG 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "COST 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "CSCO 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "CSX 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "FISV 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "GILD 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "HON 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "INTC 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "INTU 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "ISRG 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "LRCX 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "MSFT 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "MU 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "NFLX 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "ORLY 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "PEP 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "QCOM 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "REGN 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "SBUX 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "SNPS 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "TXN 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n",
      "VRTX 2002-09-03 00:00:00 2022-12-30 00:00:00 5119 5119 5119 5119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "make_2D_pretrain_data(nasdaq_list, '2002-09-03','2023-05-01',save_option = True, country = 'NASDAQ') #2002-09-03\n",
    "# print()\n",
    "# make_2D_pretrain_data(djia_list, '2004-09-30', '2023-05-01', save_option = True, country = 'DJIA') # 2002-08-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_price_label(directory):\n",
    "    # directory = f\"/home/ubuntu2010/바탕화면/famafrench_data/stockdata/label_data/{market}\"\n",
    "    filedir = f\"{directory}/x/stock_2d_actor.csv\"\n",
    "\n",
    "    data = pd.read_csv(filedir, index_col='date', parse_dates=['date'])\n",
    "\n",
    "    close_df = data[[i for i in data.columns if 'close' in i]]\n",
    "    y_data = close_df.shift(-1).fillna(method='ffill').fillna(method='bfill')\n",
    "    data_logr_whole= np.log(data/data.shift(1).fillna(method='bfill'))\n",
    "    data_logr_whole = data_logr_whole.fillna(method='ffill')\n",
    "    data_log_close = np.log(close_df/close_df.shift(1).fillna(method='bfill'))\n",
    "    data_log_close = data_log_close.fillna(method='ffill')\n",
    "    data_logr_y = data_log_close.shift(-1).fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    print(\"# of Null data: \", close_df.isna().sum().sum()) # no null\n",
    "    print(\"# of Null data: \", y_data.isna().sum().sum()) # no null\n",
    "    print(\"# of Null data: \", data_logr_whole.isna().sum().sum()) # no null\n",
    "    print(\"# of Null data: \", data_log_close.isna().sum().sum()) # no null\n",
    "    \n",
    "    print(\"# of data: \", len(close_df)) # no null\n",
    "    print(\"# of data: \", len(y_data)) # no null\n",
    "    print(\"# of data: \", len(data_logr_whole)) # no null\n",
    "    print(\"# of data: \", len(data_log_close)) # no null\n",
    "\n",
    "    # file save'{DATA_DIR}/stockdata/label_data/{market}\n",
    "    # origin price data\n",
    "    close_df.to_csv(f\"{directory}/x/stock_2d_onlyclose.csv\")\n",
    "    y_data.to_csv(f\"{directory}/y/stock_2d_origin_price.csv\")\n",
    "\n",
    "    # log returns data \n",
    "    data_logr_whole.to_csv(f\"{directory}/x/stock_2d_log_all.csv\")\n",
    "    data_log_close.to_csv(f\"{directory}/x/stock_2d_log_close.csv\")\n",
    "    data_logr_y.to_csv(f\"{directory}/y/stock_2d_logr_y.csv\")\n",
    "    \n",
    "    return close_df, y_data, data_logr_whole, data_log_close, data_logr_y\n",
    "\n",
    "\n",
    "\n",
    "def make_pretrain_dataset(directory, market:str):\n",
    "    directory = directory+f\"/{market}\"\n",
    "    filedir = f\"{directory}/x/stock_2d_actor.csv\"\n",
    "\n",
    "    pretrain_datadir = directory+f\"/pretrain\"\n",
    "    os.makedirs(pretrain_datadir, exist_ok=True)\n",
    "    close_df, y_data, data_logr_whole, data_log_close, data_logr_y = make_price_label(directory)\n",
    "    data_dict = {\"close\":close_df, \"log_p\":data_log_close, \"label\":y_data, \"log_label\":data_logr_y}\n",
    "    tickers = [i.split(\"_\")[-1] for i in y_data.columns]\n",
    "    print(\"tickers: \", tickers)\n",
    "    # container = {}\n",
    "\n",
    "    for tic in tickers:\n",
    "        container = []\n",
    "        for col_name, df in data_dict.items():\n",
    "            # print(col_name)\n",
    "            for col in df.columns:\n",
    "                if tic == col.split(\"_\")[-1]: container.append(df[col])\n",
    "        print(tic)\n",
    "        final_df = pd.DataFrame(container)\n",
    "        final_df = final_df.transpose()\n",
    "        final_df.columns = [\"close\", \"log_p\", \"label\", \"log_label\"]\n",
    "        final_df.to_csv(pretrain_datadir+f\"/{tic}_pretrain_dataset.csv\")\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hwang/anaconda3/envs/famafrench/lib/python3.8/site-packages/pandas/core/internals/blocks.py:352: RuntimeWarning: divide by zero encountered in log\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Null data:  0\n",
      "# of Null data:  0\n",
      "# of Null data:  0\n",
      "# of Null data:  0\n",
      "# of data:  5119\n",
      "# of data:  5119\n",
      "# of data:  5119\n",
      "# of data:  5119\n",
      "tickers:  ['AAPL', 'ADBE', 'ADI', 'ADP', 'AMAT', 'AMD', 'AMGN', 'AMZN', 'BKNG', 'COST', 'CSCO', 'CSX', 'FISV', 'GILD', 'HON', 'INTC', 'INTU', 'ISRG', 'LRCX', 'MSFT', 'MU', 'NFLX', 'ORLY', 'PEP', 'QCOM', 'REGN', 'SBUX', 'SNPS', 'TXN', 'VRTX']\n",
      "AAPL\n",
      "ADBE\n",
      "ADI\n",
      "ADP\n",
      "AMAT\n",
      "AMD\n",
      "AMGN\n",
      "AMZN\n",
      "BKNG\n",
      "COST\n",
      "CSCO\n",
      "CSX\n",
      "FISV\n",
      "GILD\n",
      "HON\n",
      "INTC\n",
      "INTU\n",
      "ISRG\n",
      "LRCX\n",
      "MSFT\n",
      "MU\n",
      "NFLX\n",
      "ORLY\n",
      "PEP\n",
      "QCOM\n",
      "REGN\n",
      "SBUX\n",
      "SNPS\n",
      "TXN\n",
      "VRTX\n"
     ]
    }
   ],
   "source": [
    "# price_filename = f\"{DATA_DIR}/stockdata/label_data/{market}/x/stock_2d_actor.csv\"\n",
    "directory = f\"{DATA_DIR}/stockdata/label_data\"\n",
    "\n",
    "nasq_df = make_pretrain_dataset(directory=directory, market='NASDAQ')\n",
    "# djia_df = make_pretrain_dataset(directory=directory, market='DJIA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습시 env 설정을 위한 jsonfile만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON File도 만들어야 하는데. 그거는 좀이따\n",
    "portfolio_info_dir = f'{BASE_DIR}/portfolio_info'\n",
    "json_name = 'portfolio.json' #'diy_portfolio.json'\n",
    "\n",
    "os.makedirs(portfolio_info_dir+f\"/NASDAQ\", exist_ok=True)\n",
    "os.makedirs(portfolio_info_dir+f\"/DJIA\", exist_ok=True)\n",
    "\n",
    "nasq_df = pd.read_csv(f\"{DATA_DIR}/stockdata/label_data/NASDAQ/x/stock_2d_actor.csv\", index_col='date', parse_dates=['date'])\n",
    "djia_df = pd.read_csv(f\"{DATA_DIR}/stockdata/label_data/DJIA/x/stock_2d_actor.csv\", index_col='date', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def make_pfsetting_json(df, market='NASDAQ'):\n",
    "    container = {}\n",
    "    init_pf_dict = {\"Bank_account\": 100000} \n",
    "    cap_dict = {}\n",
    "\n",
    "    # df의 marketcap 기준으로 Small, Mid, Big 계산\n",
    "    # 우선은 평균으로 계산.. 이것도 바꿀 수 있음 좋지만..\n",
    "    cap_df = df[[i for i in df.columns if 'Market' in i]]\n",
    "    cap_df_desc = cap_df.describe().T\n",
    "    # print(cap_df_desc)\n",
    "    cap_df_desc = cap_df_desc.sort_values(by=\"mean\", ascending=False)\n",
    "    sorted_ticker_list = [i.split(\"_\")[-1] for i in cap_df_desc.index] #마켓캡 크기순\n",
    "    ticker_list = sorted(sorted_ticker_list) #abc순\n",
    "\n",
    "    for i in range(len(sorted_ticker_list)):\n",
    "        tic = sorted_ticker_list[i]\n",
    "        abc_tic = ticker_list[i]\n",
    "        init_pf_dict[abc_tic] = 0\n",
    "\n",
    "        if i <= int(len(sorted_ticker_list)/3-1): cap_dict[tic] = 'Big'\n",
    "        elif i > int(len(sorted_ticker_list)*2/3): cap_dict[tic] = 'Small'\n",
    "        else : cap_dict[tic] = 'Mid'\n",
    "    \n",
    "    container['initial_portfolio'] = init_pf_dict\n",
    "    container['market_cap'] = dict(sorted(cap_dict.items()))\n",
    "    # print(container)\n",
    "\n",
    "    # save\n",
    "    portfolio_info_dir = f'{BASE_DIR}/portfolio_info'\n",
    "    json_name = 'snp_portfolio_smb.json' #'diy_portfolio.json'\n",
    "    with open(f'{portfolio_info_dir}/{market}/{json_name}','w') as f:\n",
    "        json.dump(container, f, indent=4)\n",
    "    return container\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'initial_portfolio': {'Bank_account': 100000,\n",
       "  'AAPL': 0,\n",
       "  'AMGN': 0,\n",
       "  'AXP': 0,\n",
       "  'BA': 0,\n",
       "  'CAT': 0,\n",
       "  'CRM': 0,\n",
       "  'CSCO': 0,\n",
       "  'CVX': 0,\n",
       "  'DIS': 0,\n",
       "  'DOW': 0,\n",
       "  'GS': 0,\n",
       "  'HD': 0,\n",
       "  'HON': 0,\n",
       "  'IBM': 0,\n",
       "  'INTC': 0,\n",
       "  'JNJ': 0,\n",
       "  'JPM': 0,\n",
       "  'KO': 0,\n",
       "  'MCD': 0,\n",
       "  'MMM': 0,\n",
       "  'MRK': 0,\n",
       "  'MSFT': 0,\n",
       "  'NKE': 0,\n",
       "  'PG': 0,\n",
       "  'TRV': 0,\n",
       "  'UNH': 0,\n",
       "  'V': 0,\n",
       "  'VZ': 0,\n",
       "  'WBA': 0,\n",
       "  'WMT': 0},\n",
       " 'market_cap': {'AAPL': 'Big',\n",
       "  'AMGN': 'Mid',\n",
       "  'AXP': 'Small',\n",
       "  'BA': 'Mid',\n",
       "  'CAT': 'Small',\n",
       "  'CRM': 'Small',\n",
       "  'CSCO': 'Mid',\n",
       "  'CVX': 'Big',\n",
       "  'DIS': 'Mid',\n",
       "  'DOW': 'Small',\n",
       "  'GS': 'Small',\n",
       "  'HD': 'Mid',\n",
       "  'HON': 'Small',\n",
       "  'IBM': 'Mid',\n",
       "  'INTC': 'Mid',\n",
       "  'JNJ': 'Big',\n",
       "  'JPM': 'Big',\n",
       "  'KO': 'Big',\n",
       "  'MCD': 'Mid',\n",
       "  'MMM': 'Mid',\n",
       "  'MRK': 'Mid',\n",
       "  'MSFT': 'Big',\n",
       "  'NKE': 'Small',\n",
       "  'PG': 'Big',\n",
       "  'TRV': 'Small',\n",
       "  'UNH': 'Mid',\n",
       "  'V': 'Big',\n",
       "  'VZ': 'Big',\n",
       "  'WBA': 'Small',\n",
       "  'WMT': 'Big'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_pfsetting_json(nasq_df, market='NASDAQ')\n",
    "make_pfsetting_json(djia_df, market='DJIA')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sub datasets : DJIA 5, NASDAQ 5, 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "famafrench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e98f2eee9a898e4d686edaaf738d0c261e052aa798f48fce9d1148c32f28e070"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
